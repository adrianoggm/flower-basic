name: PR - Code Review and Testing

on:
  pull_request:
    types: [opened, synchronize, reopened]
    branches: [main, develop]

jobs:
  code-review:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch full history for better diffs

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: 3.10.11

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
          pip install pytest pytest-cov black isort flake8 mypy

      - name: Run comprehensive tests
        id: tests
        run: |
          # Create results directory
          mkdir -p pr-results

          # Run tests with detailed output
          pytest tests/ -v --tb=short --cov=. --cov-report=term-missing --cov-report=html:pr-results/coverage > pr-results/test-output.txt 2>&1
          TEST_RESULT=$?

          # Run code quality checks
          echo "=== BLACK FORMATTING ===" > pr-results/quality-report.txt
          black --check --diff . >> pr-results/quality-report.txt 2>&1 || echo "Formatting issues found" >> pr-results/quality-report.txt

          echo -e "\n=== IMPORT SORTING ===" >> pr-results/quality-report.txt
          isort --check-only --diff . >> pr-results/quality-report.txt 2>&1 || echo "Import sorting issues found" >> pr-results/quality-report.txt

          echo -e "\n=== LINTING RESULTS ===" >> pr-results/quality-report.txt
          flake8 . --statistics >> pr-results/quality-report.txt 2>&1 || echo "Linting issues found" >> pr-results/quality-report.txt

          # Extract test summary
          grep -E "(passed|failed|error)" pr-results/test-output.txt | tail -n 5 > pr-results/test-summary.txt

          # Set outputs for later use
          echo "test_result=$TEST_RESULT" >> $GITHUB_OUTPUT

          exit 0  # Don't fail the job, we'll report in comments

      - name: Generate PR comment
        id: pr-comment
        run: |
          # Create comprehensive PR comment
          cat > pr-comment.md << 'EOF'
          ## üîç Code Review Results

          ### üìä Test Results
          ```
          $(cat pr-results/test-summary.txt)
          ```

          ### üìã Code Quality Report
          ```
          $(head -n 50 pr-results/quality-report.txt)
          ```

          ### üéØ Quick Actions
          - **Format code**: `black . && isort .`
          - **Run tests**: `pytest tests/ -v`
          - **Check coverage**: `pytest tests/ --cov=.`

          ---
          *Automated review by GitHub Actions* ü§ñ
          EOF

          # Count test results
          PASSED=$(grep -c "PASSED" pr-results/test-output.txt || echo "0")
          FAILED=$(grep -c "FAILED" pr-results/test-output.txt || echo "0")

          # Create status summary
          if [ "$FAILED" -eq "0" ]; then
            echo "status=‚úÖ All tests passing ($PASSED/$(($PASSED + $FAILED)))" >> $GITHUB_OUTPUT
          else
            echo "status=‚ùå Some tests failing ($PASSED/$(($PASSED + $FAILED)))" >> $GITHUB_OUTPUT
          fi

      - name: Comment on PR
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('pr-comment.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Upload test artifacts
        uses: actions/upload-artifact@v3
        with:
          name: pr-test-results-${{ github.event.number }}
          path: pr-results/
          retention-days: 7

  compatibility-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Test compatibility
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest

          # Run basic compatibility tests
          python -c "
          try:
              from flower_basic import ECGModel
              from flower_basic import load_ecg5000_openml, state_dict_to_numpy
              from flower_basic import weighted_average
              print(f'‚úÖ Python {python_version} compatibility: OK')
          except Exception as e:
              print(f'‚ùå Python {python_version} compatibility: FAILED - {e}')
              exit(1)
          " python_version=${{ matrix.python-version }}

  security-check:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: "fs"
          scan-ref: "."
          format: "sarif"
          output: "trivy-results.sarif"

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: "trivy-results.sarif"

  performance-regression:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: 3.10.11

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark memory-profiler

      - name: Run performance regression tests
        run: |
          python -c "
          import time
          import tracemalloc
          from flower_basic import ECGModel
          from flower_basic import get_parameters, set_parameters
          import numpy as np

          print('=== Performance Regression Test ===')

          # Memory profiling
          tracemalloc.start()

          # Model operations benchmark
          model = ECGModel()

          # Parameter extraction benchmark
          start_time = time.time()
          for _ in range(100):
              params = get_parameters(model)
          param_time = time.time() - start_time

          # Parameter setting benchmark  
          start_time = time.time()
          for _ in range(100):
              set_parameters(model, params)
          set_time = time.time() - start_time

          current, peak = tracemalloc.get_traced_memory()
          tracemalloc.stop()

          print(f'Parameter extraction: {param_time:.3f}s (100 ops)')
          print(f'Parameter setting: {set_time:.3f}s (100 ops)')
          print(f'Peak memory usage: {peak / 1024 / 1024:.2f}MB')

          # Performance assertions
          assert param_time < 1.0, 'Parameter extraction regression detected'
          assert set_time < 1.0, 'Parameter setting regression detected'
          assert peak < 100 * 1024 * 1024, 'Memory usage regression detected'

          print('‚úÖ No performance regressions detected')
          "

  pr-summary:
    runs-on: ubuntu-latest
    needs:
      [code-review, compatibility-test, security-check, performance-regression]
    if: always()

    steps:
      - name: PR Summary
        run: |
          echo "## üìã PR Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Review | ${{ needs.code-review.result == 'success' && '‚úÖ' || '‚ùå' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Compatibility | ${{ needs.compatibility-test.result == 'success' && '‚úÖ' || '‚ùå' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security | ${{ needs.security-check.result == 'success' && '‚úÖ' || '‚ùå' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance | ${{ needs.performance-regression.result == 'success' && '‚úÖ' || '‚ùå' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ "${{ needs.code-review.result }}" == "success" && "${{ needs.compatibility-test.result }}" == "success" ]]; then
            echo "üéâ **PR is ready for review!**" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è **PR needs attention before merge**" >> $GITHUB_STEP_SUMMARY
          fi
